2022.01.13 01:03:21 INFO  es[][o.e.n.Node] version[7.16.2], pid[38], build[default/tar/2b937c44140b6559905130a8650c64dbd0879cfb/2021-12-18T19:42:46.604893745Z], OS[Linux/5.10.60.1-microsoft-standard-WSL2/amd64], JVM[Alpine/OpenJDK 64-Bit Server VM/11.0.12/11.0.12+7-alpine-r0]
2022.01.13 01:03:21 INFO  es[][o.e.n.Node] JVM home [/usr/lib/jvm/java-11-openjdk]
2022.01.13 01:03:21 INFO  es[][o.e.n.Node] JVM arguments [-XX:+UseG1GC, -Djava.io.tmpdir=/opt/sonarqube/temp, -XX:ErrorFile=../logs/es_hs_err_pid%p.log, -Des.networkaddress.cache.ttl=60, -Des.networkaddress.cache.negative.ttl=10, -XX:+AlwaysPreTouch, -Xss1m, -Djava.awt.headless=true, -Dfile.encoding=UTF-8, -Djna.nosys=true, -Djna.tmpdir=/opt/sonarqube/temp, -XX:-OmitStackTraceInFastThrow, -Dio.netty.noUnsafe=true, -Dio.netty.noKeySetOptimization=true, -Dio.netty.recycler.maxCapacityPerThread=0, -Dio.netty.allocator.numDirectArenas=0, -Dlog4j.shutdownHookEnabled=false, -Dlog4j2.disable.jmx=true, -Dlog4j2.formatMsgNoLookups=true, -Djava.locale.providers=COMPAT, -Dcom.redhat.fips=false, -Xmx512m, -Xms512m, -XX:MaxDirectMemorySize=256m, -XX:+HeapDumpOnOutOfMemoryError, -Des.path.home=/opt/sonarqube/elasticsearch, -Des.path.conf=/opt/sonarqube/temp/conf/es, -Des.distribution.flavor=default, -Des.distribution.type=tar, -Des.bundled_jdk=false]
2022.01.13 01:03:22 INFO  es[][o.e.p.PluginsService] loaded module [analysis-common]
2022.01.13 01:03:22 INFO  es[][o.e.p.PluginsService] loaded module [lang-painless]
2022.01.13 01:03:22 INFO  es[][o.e.p.PluginsService] loaded module [parent-join]
2022.01.13 01:03:22 INFO  es[][o.e.p.PluginsService] loaded module [transport-netty4]
2022.01.13 01:03:22 INFO  es[][o.e.p.PluginsService] no plugins loaded
2022.01.13 01:03:22 INFO  es[][o.e.e.NodeEnvironment] using [1] data paths, mounts [[/opt/sonarqube/data (C:\134)]], net usable_space [344.5gb], net total_space [476.1gb], types [9p]
2022.01.13 01:03:22 INFO  es[][o.e.e.NodeEnvironment] heap size [512mb], compressed ordinary object pointers [true]
2022.01.13 01:03:22 INFO  es[][o.e.n.Node] node name [sonarqube], node ID [OokrsvIiS16rKkddJWLyGg], cluster name [sonarqube], roles [data_frozen, master, remote_cluster_client, data, data_content, data_hot, data_warm, data_cold, ingest]
2022.01.13 01:03:30 INFO  es[][o.e.t.NettyAllocator] creating NettyAllocator with the following configs: [name=unpooled, suggested_max_allocation_size=256kb, factors={es.unsafe.use_unpooled_allocator=null, g1gc_enabled=true, g1gc_region_size=1mb, heap_size=512mb}]
2022.01.13 01:03:30 INFO  es[][o.e.d.DiscoveryModule] using discovery type [zen] and seed hosts providers [settings]
2022.01.13 01:03:31 INFO  es[][o.e.g.DanglingIndicesState] gateway.auto_import_dangling_indices is disabled, dangling indices will not be automatically detected or imported and must be managed manually
2022.01.13 01:03:31 INFO  es[][o.e.n.Node] initialized
2022.01.13 01:03:31 INFO  es[][o.e.n.Node] starting ...
2022.01.13 01:03:31 INFO  es[][o.e.t.TransportService] publish_address {127.0.0.1:35093}, bound_addresses {127.0.0.1:35093}
2022.01.13 01:03:32 WARN  es[][o.e.b.BootstrapChecks] max virtual memory areas vm.max_map_count [65530] is too low, increase to at least [262144]
2022.01.13 01:03:32 INFO  es[][o.e.c.c.Coordinator] setting initial configuration to VotingConfiguration{OokrsvIiS16rKkddJWLyGg}
2022.01.13 01:03:32 INFO  es[][o.e.c.s.MasterService] elected-as-master ([1] nodes joined)[{sonarqube}{OokrsvIiS16rKkddJWLyGg}{dz4tAAAbTY28ms0S6WNaDw}{127.0.0.1}{127.0.0.1:35093}{cdfhimrsw} elect leader, _BECOME_MASTER_TASK_, _FINISH_ELECTION_], term: 1, version: 1, delta: master node changed {previous [], current [{sonarqube}{OokrsvIiS16rKkddJWLyGg}{dz4tAAAbTY28ms0S6WNaDw}{127.0.0.1}{127.0.0.1:35093}{cdfhimrsw}]}
2022.01.13 01:03:32 INFO  es[][o.e.c.c.CoordinationState] cluster UUID set to [rRZxIi1nQzKvrr6DbnhHKA]
2022.01.13 01:03:32 INFO  es[][o.e.c.s.ClusterApplierService] master node changed {previous [], current [{sonarqube}{OokrsvIiS16rKkddJWLyGg}{dz4tAAAbTY28ms0S6WNaDw}{127.0.0.1}{127.0.0.1:35093}{cdfhimrsw}]}, term: 1, version: 1, reason: Publication{term=1, version=1}
2022.01.13 01:03:33 INFO  es[][o.e.h.AbstractHttpServerTransport] publish_address {127.0.0.1:9001}, bound_addresses {127.0.0.1:9001}
2022.01.13 01:03:33 INFO  es[][o.e.n.Node] started
2022.01.13 01:03:33 INFO  es[][o.e.g.GatewayService] recovered [0] indices into cluster_state
2022.01.13 01:04:16 INFO  es[][o.e.c.m.MetadataCreateIndexService] [metadatas] creating index, cause [api], templates [], shards [1]/[0]
2022.01.13 01:04:17 INFO  es[][o.e.c.r.a.AllocationService] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[metadatas][0]]]).
2022.01.13 01:04:17 INFO  es[][o.e.c.m.MetadataMappingService] [metadatas/taPSaGoCROOPkhQkBlPQBA] create_mapping [metadata]
2022.01.13 01:04:18 INFO  es[][o.e.c.m.MetadataCreateIndexService] [components] creating index, cause [api], templates [], shards [5]/[0]
2022.01.13 01:04:20 INFO  es[][o.e.c.r.a.AllocationService] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[components][4]]]).
2022.01.13 01:04:20 INFO  es[][o.e.c.m.MetadataMappingService] [components/7uXncOX7QYekojER7P0lwQ] create_mapping [auth]
2022.01.13 01:04:21 INFO  es[][o.e.c.m.MetadataCreateIndexService] [projectmeasures] creating index, cause [api], templates [], shards [5]/[0]
2022.01.13 01:04:23 INFO  es[][o.e.c.r.a.AllocationService] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[projectmeasures][4]]]).
2022.01.13 01:04:23 INFO  es[][o.e.c.m.MetadataMappingService] [projectmeasures/jcj0EkwHSfey0CH0s81b3w] create_mapping [auth]
2022.01.13 01:04:24 INFO  es[][o.e.c.m.MetadataCreateIndexService] [rules] creating index, cause [api], templates [], shards [2]/[0]
2022.01.13 01:04:24 INFO  es[][o.e.c.r.a.AllocationService] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[rules][0]]]).
2022.01.13 01:04:25 INFO  es[][o.e.c.m.MetadataMappingService] [rules/-AQv-w0NRiuA-LPvSzpEcw] create_mapping [rule]
2022.01.13 01:04:25 INFO  es[][o.e.c.m.MetadataCreateIndexService] [issues] creating index, cause [api], templates [], shards [5]/[0]
2022.01.13 01:04:27 INFO  es[][o.e.c.r.a.AllocationService] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[issues][4]]]).
2022.01.13 01:04:27 INFO  es[][o.e.c.m.MetadataMappingService] [issues/Q3kNHxYmQOqoFohzCibDHg] create_mapping [auth]
2022.01.13 01:04:28 INFO  es[][o.e.c.m.MetadataCreateIndexService] [users] creating index, cause [api], templates [], shards [1]/[0]
2022.01.13 01:04:28 INFO  es[][o.e.c.r.a.AllocationService] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[users][0]]]).
2022.01.13 01:04:28 INFO  es[][o.e.c.m.MetadataMappingService] [users/Yd1pNr8YROGP0lXI_1-Tsg] create_mapping [user]
2022.01.13 01:04:29 INFO  es[][o.e.c.m.MetadataCreateIndexService] [views] creating index, cause [api], templates [], shards [5]/[0]
2022.01.13 01:04:31 INFO  es[][o.e.c.r.a.AllocationService] Cluster health status changed from [YELLOW] to [GREEN] (reason: [shards started [[views][4]]]).
2022.01.13 01:04:31 INFO  es[][o.e.c.m.MetadataMappingService] [views/q6Gh_tJRQzqmKcZCPf3YmA] create_mapping [view]
2022.01.13 01:05:12 INFO  es[][o.e.c.s.IndexScopedSettings] updating [index.refresh_interval] from [30s] to [-1]
2022.01.13 01:05:13 INFO  es[][o.e.c.s.IndexScopedSettings] updating [index.refresh_interval] from [30s] to [-1]
2022.01.13 01:05:13 INFO  es[][o.e.c.s.IndexScopedSettings] updating [index.refresh_interval] from [-1] to [30s]
2022.01.13 01:05:13 INFO  es[][o.e.c.s.IndexScopedSettings] updating [index.refresh_interval] from [-1] to [30s]
2022.01.13 01:05:14 INFO  es[][o.e.c.s.IndexScopedSettings] updating [index.refresh_interval] from [30s] to [-1]
2022.01.13 01:05:15 INFO  es[][o.e.c.s.IndexScopedSettings] updating [index.refresh_interval] from [30s] to [-1]
2022.01.13 01:05:15 INFO  es[][o.e.c.s.IndexScopedSettings] updating [index.refresh_interval] from [-1] to [30s]
2022.01.13 01:05:15 INFO  es[][o.e.c.s.IndexScopedSettings] updating [index.refresh_interval] from [-1] to [30s]
2022.01.13 02:22:25 WARN  es[][o.e.t.ThreadPool] timer thread slept for [9.3s/9316ms] on absolute clock which is above the warn threshold of [5000ms]
2022.01.13 10:55:03 WARN  es[][o.e.t.ThreadPool] timer thread slept for [9.3s/9316104918ns] on relative clock which is above the warn threshold of [5000ms]
2022.01.13 10:55:03 WARN  es[][o.e.t.ThreadPool] timer thread slept for [8.5h/30757745ms] on absolute clock which is above the warn threshold of [5000ms]
2022.01.13 11:55:41 WARN  es[][o.e.t.ThreadPool] timer thread slept for [1.4m/84927ms] on absolute clock which is above the warn threshold of [5000ms]
2022.01.13 11:55:42 WARN  es[][o.e.t.ThreadPool] timer thread slept for [1.4m/84927303112ns] on relative clock which is above the warn threshold of [5000ms]
2022.01.13 16:36:49 WARN  es[][o.e.t.ThreadPool] timer thread slept for [3m/185423ms] on absolute clock which is above the warn threshold of [5000ms]
2022.01.13 16:36:50 WARN  es[][o.e.t.ThreadPool] timer thread slept for [3m/185423153229ns] on relative clock which is above the warn threshold of [5000ms]
2022.01.13 17:35:41 WARN  es[][o.e.t.ThreadPool] timer thread slept for [4.6m/276815ms] on absolute clock which is above the warn threshold of [5000ms]
2022.01.13 17:35:41 WARN  es[][o.e.t.ThreadPool] timer thread slept for [4.6m/276815082425ns] on relative clock which is above the warn threshold of [5000ms]
2022.01.13 17:56:02 WARN  es[][o.e.t.ThreadPool] timer thread slept for [12.3m/743914ms] on absolute clock which is above the warn threshold of [5000ms]
2022.01.13 17:56:03 WARN  es[][o.e.t.ThreadPool] timer thread slept for [12.3m/743914521462ns] on relative clock which is above the warn threshold of [5000ms]
2022.01.13 18:18:21 WARN  es[][o.e.t.ThreadPool] timer thread slept for [57.6s/57687ms] on absolute clock which is above the warn threshold of [5000ms]
2022.01.13 18:18:21 WARN  es[][o.e.t.ThreadPool] execution of [ReschedulingRunnable{runnable=org.elasticsearch.indices.IndexingMemoryController$ShardsIndicesStatusChecker@4ad9b95f, interval=5s}] took [57686ms] which is above the warn threshold of [5000ms]
2022.01.13 18:18:22 WARN  es[][o.e.t.ThreadPool] timer thread slept for [57.6s/57686547176ns] on relative clock which is above the warn threshold of [5000ms]
2022.01.13 18:23:35 WARN  es[][o.e.t.ThreadPool] timer thread slept for [11.1s/11186ms] on absolute clock which is above the warn threshold of [5000ms]
2022.01.13 18:23:35 WARN  es[][o.e.t.ThreadPool] timer thread slept for [11.1s/11185605560ns] on relative clock which is above the warn threshold of [5000ms]
2022.01.13 19:46:44 WARN  es[][o.e.t.ThreadPool] timer thread slept for [2.3m/141323ms] on absolute clock which is above the warn threshold of [5000ms]
2022.01.13 19:46:45 WARN  es[][o.e.t.ThreadPool] timer thread slept for [2.3m/141322683546ns] on relative clock which is above the warn threshold of [5000ms]
2022.01.13 21:13:54 WARN  es[][o.e.t.ThreadPool] timer thread slept for [13.5s/13545ms] on absolute clock which is above the warn threshold of [5000ms]
